{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot - Seq2Seq LSTM Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMcz8sAX8yA/lR1k/Qh0Des",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khldsqmr/Comparison-of-Chatbot-models/blob/main/Model%201%3A%20Chatbot_Seq2Seq_LSTM_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rifrBi0Kh3m"
      },
      "source": [
        "# **Seq2Seq Encoder Decoder LSTM Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be7M428FW4bK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "492138e9-246d-4815-d30f-936633e782bc"
      },
      "source": [
        "#Import Libraries\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import string\n",
        "from string import digits\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from keras.layers import Input, LSTM, Embedding, Dense,Dropout,TimeDistributed\n",
        "from keras.models import Model\n",
        "\n",
        "def main(data_path):\n",
        "\n",
        "    #Read the Files\n",
        "    with open(os.path.join(data_path, 'movie_lines.txt'), encoding = 'utf-8', errors = 'ignore') as f:\n",
        "      movieLines = f.read().split('\\n')\n",
        "    with open(os.path.join(data_path, 'movie_conversations.txt'), encoding = 'utf-8', errors = 'ignore') as f:\n",
        "      movieConversations = f.read().split('\\n')\n",
        "\n",
        "#Read the Files\n",
        "#movieLines = open('movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
        "#movieConversations = open('movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
        "\n",
        "    #Data Preprocessing\n",
        "    #Map each line's id with its text by creating a dictionary\n",
        "    id2line = {}\n",
        "    for l in movieLines:\n",
        "        L = l.split(' +++$+++ ')\n",
        "        if len(L) == 5:\n",
        "            id2line[L[0]] = L[4]\n",
        "\n",
        "    #Create a list of all of the conversations' lines' ids.\n",
        "    conv_ids = []\n",
        "    for c in movieConversations[:-1]:\n",
        "        C = c.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
        "        conv_ids.append(C.split(','))\n",
        "\n",
        "    #Sort the sentences into questions (inputs) and answers (targets)\n",
        "    questions = []\n",
        "    answers = []\n",
        "    for c in conv_ids:\n",
        "        for i in range(len(c) - 1):\n",
        "            questions.append(id2line[c[i]])\n",
        "            answers.append(id2line[c[i+1]])\n",
        "\n",
        "    #Print length of question set and answer set\n",
        "    print('Total Number of questions: ', len(questions))\n",
        "    print('Total Number of answers  :', len(answers))\n",
        "\n",
        "    #Remove Punctuations\n",
        "    def removePuncAndClean(txt):\n",
        "        txt = txt.lower()\n",
        "        txt = re.sub(r\"i'm\", \"i am\", txt)\n",
        "        txt = re.sub(r\"he's\", \"he is\", txt)\n",
        "        txt = re.sub(r\"she's\", \"she is\", txt)\n",
        "        txt = re.sub(r\"it's\", \"it is\", txt)\n",
        "        txt = re.sub(r\"that's\", \"that is\", txt)\n",
        "        txt = re.sub(r\"what's\", \"that is\", txt)\n",
        "        txt = re.sub(r\"where's\", \"where is\", txt)\n",
        "        txt = re.sub(r\"how's\", \"how is\", txt)\n",
        "        txt = re.sub(r\"\\'ll\", \" will\", txt)\n",
        "        txt = re.sub(r\"\\'ve\", \" have\", txt)\n",
        "        txt = re.sub(r\"\\'re\", \" are\", txt)\n",
        "        txt = re.sub(r\"\\'d\", \" would\", txt)\n",
        "        txt = re.sub(r\"won't\", \"will not\", txt)\n",
        "        txt = re.sub(r\"can't\", \"cannot\", txt)\n",
        "        txt = re.sub(r\"n't\", \" not\", txt)\n",
        "        txt = re.sub(r\"n'\", \"ng\", txt)\n",
        "        txt = re.sub(r\"'bout\", \"about\", txt)\n",
        "        txt = re.sub(r\"'til\", \"until\", txt)\n",
        "        txt = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|]\", \"\", txt)\n",
        "\n",
        "        return txt\n",
        "\n",
        "    #Clean the data\n",
        "    cleanQuestions = []\n",
        "    for q in questions:\n",
        "        cleanQuestions.append(removePuncAndClean(q))\n",
        "\n",
        "    cleanAnswers = []\n",
        "    for a in answers:\n",
        "        cleanAnswers.append(removePuncAndClean(a))\n",
        "\n",
        "    #Remove questions and answers that are shorter than 1 word and longer than 25 words.\n",
        "    smallQuestions = []\n",
        "    smallAnswers = []\n",
        "    i = 0\n",
        "    for q in cleanQuestions:\n",
        "        if len(q.split()) >= 2 and len(q.split()) <= 25:\n",
        "            smallQuestions.append(q)\n",
        "            smallAnswers.append(cleanAnswers[i])\n",
        "        i += 1\n",
        "    cleanQuestions = []\n",
        "    cleanAnswers = []\n",
        "    i = 0\n",
        "    for a in smallAnswers:\n",
        "        if len(a.split()) >= 2 and len(a.split()) <= 25:\n",
        "            cleanAnswers.append(a)\n",
        "            cleanQuestions.append(smallQuestions[i])\n",
        "        i += 1\n",
        "\n",
        "    cleanQuestions = list(cleanQuestions)\n",
        "    cleanAnswers = list(cleanAnswers)\n",
        "\n",
        "    inputQuestions = []\n",
        "    targetQuestions = []\n",
        "\n",
        "    #Set sample size\n",
        "    SampleSize = 15000\n",
        "\n",
        "    for x in cleanQuestions[:SampleSize]:\n",
        "      inputQuestions.append(x)\n",
        "\n",
        "    for x in cleanAnswers[:SampleSize]:\n",
        "      targetQuestions.append(x)\n",
        "\n",
        "    #Convert to dataframe\n",
        "    dfData =  list(zip(inputQuestions, targetQuestions))\n",
        "    data = pd.DataFrame(dfData, columns = ['input' , 'target'])\n",
        "\n",
        "    # Add start and end tokens to target sequences\n",
        "    data.target = data.target.apply(lambda x : 'START '+ x + ' END')\n",
        "    print('---')\n",
        "    print('Random Sample Data: ')\n",
        "    print(data.sample(6))\n",
        "    print('---')\n",
        "\n",
        "    #Build Vocabulary\n",
        "    #Vocabulary of Questions storing all the words in a set\n",
        "    allInputWords=set()\n",
        "    for i in data.input:\n",
        "        for word in i.split():\n",
        "            if word not in allInputWords:\n",
        "                allInputWords.add(word)\n",
        "    # Vocabulary of Answers  storing all the words in a set\n",
        "    allTargetWords=set()\n",
        "    for t in data.target:\n",
        "        for word in t.split():\n",
        "            if word not in allTargetWords:\n",
        "                allTargetWords.add(word)\n",
        "\n",
        "    # Max Length of questions \n",
        "    lenList=[]\n",
        "    for l in data.input:\n",
        "        lenList.append(len(l.split(' ')))\n",
        "    maxInputLen = np.max(lenList)\n",
        "    print(\"Maximum Question's Length: \", maxInputLen)\n",
        "\n",
        "    # Max Length of Answers\n",
        "    lenList=[]\n",
        "    for l in data.target:\n",
        "        lenList.append(len(l.split(' ')))\n",
        "    maxTargetLen = np.max(lenList)\n",
        "    print(\"Maximum Answer's Length: \", maxTargetLen)\n",
        "\n",
        "    inpWords = sorted(list(allInputWords))\n",
        "    tarWords = sorted(list(allTargetWords))\n",
        "\n",
        "    #Storing the Vocabulary size for Encoder and Decoder\n",
        "    numOfEncTokens = len(allInputWords)\n",
        "    numOfDecTokens = len(allTargetWords)\n",
        "    print(\"Encoder token size is {} and decoder token size is {}\".format(numOfEncTokens,numOfDecTokens))\n",
        "\n",
        "    #Zero Padding\n",
        "    numOfDecTokens += 1 \n",
        "    print(\"Decoder Token size after zero padding: \", numOfDecTokens)\n",
        "\n",
        "    #Dictionary to index each word in Questions: key is index and value is word\n",
        "    inputIdx2char = {}\n",
        "    #Dictionary to get words given its index: key is word and value is index\n",
        "    inputChar2Idx = {}\n",
        "\n",
        "    for key, value in enumerate(inpWords):\n",
        "        inputIdx2char[key] = value\n",
        "        inputChar2Idx[value] = key\n",
        "\n",
        "    #Dictionary to index each word in Answers: key is index and value is word\n",
        "    outputIdx2char = {}\n",
        "    #Dictionary to get words given its index: key is word and value is index\n",
        "    outputChar2idx = {}\n",
        "    for key,value in enumerate(tarWords):\n",
        "        outputIdx2char[key] = value\n",
        "        outputChar2idx[value] = key\n",
        "\n",
        "    # Spliting our data into train and test\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X, y = data.input, data.target\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=False)\n",
        "\n",
        "    print(\"Train data size is: \", X_train.shape)\n",
        "    print(\"Test data size is : \", X_test.shape)\n",
        "\n",
        "    #Generator function to train on batch to reduce computation, increase learning and model performance\n",
        "    def generate_batch(X, y, batch_size = 128):\n",
        "        while True:\n",
        "            for j in range(0, len(X), batch_size):\n",
        "                \n",
        "                #encoder input data\n",
        "                encoderInputData = np.zeros((batch_size, maxInputLen),dtype='float32')\n",
        "                #decoder input data\n",
        "                decoderInputData = np.zeros((batch_size, maxTargetLen),dtype='float32')\n",
        "                \n",
        "                #decoder target data\n",
        "                decoderTargetData = np.zeros((batch_size, maxTargetLen, numOfDecTokens),dtype='float32')\n",
        "                \n",
        "                for i, (inputTxt, targetTxt) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "                    for txt, word in enumerate(inputTxt.split()):\n",
        "                        encoderInputData[i, txt] = inputChar2Idx[word] # encoder input seq\n",
        "                        \n",
        "                    for txt, word in enumerate(targetTxt.split()):\n",
        "                        if txt<len(targetTxt.split())-1:\n",
        "                            decoderInputData[i, txt] = outputChar2idx[word] # decoder input seq\n",
        "                        if txt>0:\n",
        "                            decoderTargetData[i, txt - 1, outputChar2idx[word]] = 1\n",
        "                            \n",
        "                yield([encoderInputData, decoderInputData], decoderTargetData)\n",
        "\n",
        "\n",
        "    neuronDim = 256\n",
        "\n",
        "    #Encoder\n",
        "    encoderInputs = Input(shape=(None,))\n",
        "    encoderEmbeddings =  Embedding(numOfEncTokens, neuronDim, mask_zero = True)(encoderInputs)\n",
        "    encoderLstm = LSTM(neuronDim, return_state=True)\n",
        "    encoderOutputs, state_h, state_c = encoderLstm(encoderEmbeddings)\n",
        "    encoderStates = [state_h, state_c]\n",
        "    print('encoderLstm: ', encoderLstm)\n",
        "    print('encoderState: ', encoderStates)\n",
        "\n",
        "    #Decoder, with encoderStates as initial state\n",
        "    decoderInputs = Input(shape=(None,))\n",
        "    decoderEmbeddings = Embedding(numOfDecTokens, neuronDim, mask_zero = True)\n",
        "    decEmb = decoderEmbeddings(decoderInputs)\n",
        "    #To use the return states in inference.\n",
        "    decoderLstm = LSTM(neuronDim, return_sequences=True, return_state=True)\n",
        "    decoderOutputs, _, _ = decoderLstm(decEmb,initial_state=encoderStates)\n",
        "    decoderDense = Dense(numOfDecTokens, activation='softmax')\n",
        "    decoderOutputs = decoderDense(decoderOutputs)\n",
        "    print('decoderLstm: ', decoderLstm)\n",
        "    \n",
        "    #Define the model\n",
        "    model = Model([encoderInputs, decoderInputs], decoderOutputs)\n",
        "\n",
        "    #Compile the model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    trainSamples = len(X_train)\n",
        "    valSamples = len(X_test)\n",
        "    batchSize = 64\n",
        "    epochs = 50\n",
        "\n",
        "    print('---')\n",
        "    print('Printing Model Summary...')\n",
        "    print('---')\n",
        "    model.summary()\n",
        "\n",
        "    ''' \n",
        "    #UNCOMMENT TO TRAIN THE EPOCHS\n",
        "\n",
        "    for i in range(100):\n",
        "      history=model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batchSize),\n",
        "                        steps_per_epoch = trainSamples//batchSize,\n",
        "                        epochs=1,\n",
        "                        validation_data = generate_batch(X_test, y_test,    batch_size = batchSize),\n",
        "                        validation_steps = valSamples//batchSize)\n",
        "\n",
        "    model.save_weights('Enc_Dec_Weights_100_epochs.h5')\n",
        "    '''\n",
        "\n",
        "    print('Loading the Model weights...')\n",
        "\n",
        "    model.load_weights('Enc_Dec_Weights_70_epochs.h5')\n",
        "\n",
        "\n",
        "    #Encode the input sequence to get the \"Context vectors\"\n",
        "    encoderModel = Model(encoderInputs, encoderStates)\n",
        "\n",
        "    #Setting up the decoder\n",
        "    #Tensors to hold the states of the previous time-step\n",
        "    decoderStateInput_h = Input(shape=(neuronDim,))\n",
        "    decoderStateInput_c = Input(shape=(neuronDim,))\n",
        "    decoderStateInput = [decoderStateInput_h, decoderStateInput_c]\n",
        "\n",
        "    #To retrieve the embeddings of the decoder sequence\n",
        "    decoderEmbeddings2 = decoderEmbeddings (decoderInputs)\n",
        "\n",
        "    #To predict the next word in the sequence, setting the initial states to the states from the previous time-step\n",
        "    decoderOutputs2, state_h2, state_c2 = decoderLstm(decoderEmbeddings2, initial_state=decoderStateInput)\n",
        "    decoderStates2 = [state_h2, state_c2]\n",
        "\n",
        "    #A dense softmax layer to generate probability distribution over the answers vocabulary\n",
        "    decoderOutputs2 = decoderDense(decoderOutputs2)\n",
        "\n",
        "    #Final decoder model\n",
        "    decoderModel = Model([decoderInputs] + decoderStateInput,\n",
        "                        [decoderOutputs2] + decoderStates2)\n",
        "\n",
        "    #Deoode Sequence function\n",
        "    def decodeSequence(inputSequence):\n",
        "        #Encode the input as state vectors\n",
        "        statesValue = encoderModel.predict(inputSequence)\n",
        "\n",
        "        #Generate empty target sequence of length 1\n",
        "        targetSequence = np.zeros((1,1))\n",
        "\n",
        "        #Fill the first character of target sequence with the start character\n",
        "        targetSequence[0, 0] = outputChar2idx['START']\n",
        "\n",
        "        #Sampling loop for a batch of sequences assuming batch of size 1\n",
        "        toStop = False\n",
        "        decodedSentence = ''\n",
        "        while not toStop:\n",
        "            outputTokens, h, c = decoderModel.predict([targetSequence] + statesValue)\n",
        "\n",
        "            #Sample a token\n",
        "            sampledTokenIdx = np.argmax(outputTokens[0, -1, :])\n",
        "            sampledWord =outputIdx2char[sampledTokenIdx]\n",
        "            decodedSentence += ' '+ sampledWord\n",
        "\n",
        "            #Stop condition by either the max length or find stop character\n",
        "            if (sampledWord == 'END' or\n",
        "              len(decodedSentence) > 50):\n",
        "                toStop = True\n",
        "\n",
        "            #Updating the target sequence (of length 1)\n",
        "            targetSequence = np.zeros((1,1))\n",
        "            targetSequence[0, 0] = sampledTokenIdx\n",
        "\n",
        "            #Updating the states\n",
        "            statesValue = [h, c]\n",
        "\n",
        "        return decodedSentence\n",
        "    print('---')\n",
        "    #Train generator\n",
        "    trainGenerator = generate_batch(X_train[:5], y_train[:5], batch_size = 1)\n",
        "    k=-1\n",
        "    #Test Generator\n",
        "    testGenerator = generate_batch(X_test[:5], y_test[:5], batch_size=1)\n",
        "    m=-1\n",
        "    n=-1\n",
        "\n",
        "    #Print five example sentences\n",
        "    print(\"FIVE EXAMPLES: TRAIN SENTENCE PREDICTIONS: \")\n",
        "    print('---')\n",
        "    for i in range(5):\n",
        "        n+=1\n",
        "        (inputSequence, actualOutput), _ = next(trainGenerator)\n",
        "        decodedSentence = decodeSequence(inputSequence)\n",
        "        print('Question           :', X_train[n:n+1].values[0])\n",
        "        print('Actual response    :', y_train[n:n+1].values[0][6:-4])\n",
        "        print('Predicted response :', decodedSentence[:-4])\n",
        "        print('---')\n",
        "        \n",
        "    print(\"FIVE EXAMPLES: TEST SENTENCE PREDICTIONS: \")\n",
        "    print('---')\n",
        "    n=-1\n",
        "    for i in range(5):\n",
        "        n+=1\n",
        "        (inputSequence, actualOutput), _ = next(testGenerator)\n",
        "        decodedSentence = decodeSequence(inputSequence)\n",
        "        print('Question           :', X_test[n:n+1].values[0])\n",
        "        print('Actual response    :', y_test[n:n+1].values[0][6:-4])\n",
        "        print('Predicted response :', decodedSentence[:-4])\n",
        "        print('---')\n",
        "\n",
        "    #Import libraries to calculate Bleu Score\n",
        "    print('Importing libraries to calculate bleu score...')\n",
        "    import nltk\n",
        "    from nltk.translate.bleu_score import SmoothingFunction\n",
        "    from nltk.translate.bleu_score import sentence_bleu\n",
        "    c = SmoothingFunction()\n",
        "    print('---')\n",
        "    print(\"Calculating Bleu Score for Train data...\")\n",
        "    print('---')\n",
        "    bleuScoresTrain = []\n",
        "    for i in range(len(X_train[:5])):\n",
        "\n",
        "        k+=1\n",
        "        (inputSentence, actualOutput), _ = next(trainGenerator)\n",
        "        decodedSentence = decodeSequence(inputSentence)\n",
        "\n",
        "        actualOutput = y_train[k:k+1].values[0][6:-4]\n",
        "        predictedOutput = decodedSentence[:-4]\n",
        "\n",
        "        ref = actualOutput.split(' ')\n",
        "        pred = predictedOutput.split(' ')\n",
        "\n",
        "        if len(ref) >= 4 and len(pred) >= 4:\n",
        "            BLEUscore = sentence_bleu([ref], pred, smoothing_function = c.method2)\n",
        "        elif len(ref) >= 3 and len(pred) >= 3:\n",
        "            BLEUscore = sentence_bleu([ref], pred, weights = (1/3, 1/3, 1/3), smoothing_function = c.method2)\n",
        "        elif len(ref) >= 2 and len(pred) >= 2:\n",
        "            BLEUscore = sentence_bleu([ref], pred, weights = (0.5, 0.5), smoothing_function = c.method2)\n",
        "        else:\n",
        "            BLEUscore = sentence_bleu([ref], pred, weights = [1], smoothing_function = c.method2)\n",
        "        bleuScoresTrain.append(BLEUscore)\n",
        "\n",
        "    print(\"Bleu Score for Train data: \", sum(bleuScoresTrain)/float(len(bleuScoresTrain)))\n",
        "    print('---')\n",
        "    print(\"Calculating Bleu Score for Test data...\")\n",
        "\n",
        "    bleuScoresTest = []\n",
        "    for i in range(len(X_test[:5])):\n",
        "\n",
        "        m+=1\n",
        "        (inputSentence, actualOutput), _ = next(testGenerator)\n",
        "        decodedSentence = decodeSequence(inputSentence)\n",
        "\n",
        "        actualOutput = y_train[m:m+1].values[0][6:-4]\n",
        "        predictedOutput = decodedSentence[:-4]\n",
        "\n",
        "        ref = actualOutput.split(' ')\n",
        "        pred = predictedOutput.split(' ')\n",
        "\n",
        "        if len(ref) >= 4 and len(pred) >= 4:\n",
        "            BLEUscore = sentence_bleu([ref], pred, smoothing_function = c.method2)\n",
        "        elif len(ref) >= 3 and len(pred) >= 3:\n",
        "            BLEUscore = sentence_bleu([ref], pred, weights = (1/3, 1/3, 1/3), smoothing_function = c.method2)\n",
        "        elif len(ref) >= 2 and len(pred) >= 2:\n",
        "            BLEUscore = sentence_bleu([ref], pred, weights = (0.5, 0.5), smoothing_function = c.method2)\n",
        "        else:\n",
        "            BLEUscore = sentence_bleu([ref], pred, weights = [1], smoothing_function = c.method2)\n",
        "        bleuScoresTest.append(BLEUscore)\n",
        "\n",
        "    print('---')\n",
        "    print(\"Bleu Score for Test data: \", sum(bleuScoresTest)/float(len(bleuScoresTest)))\n",
        "    print('---')\n",
        "    print('END!')\n",
        "\n",
        "#MAIN FUNCTION\n",
        "if __name__ == \"__main__\":\n",
        "    print('Scanning through the corpus...')\n",
        "    #data_path = sys.argv[1]\n",
        "    data_path = '/content/'\n",
        "    main(data_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scanning through the corpus...\n",
            "Total Number of questions:  221616\n",
            "Total Number of answers  : 221616\n",
            "---\n",
            "Random Sample Data: \n",
            "                                                   input                                             target\n",
            "9512   i guess what i am trying to say is what the he...  START what do you mean what kind of person? he...\n",
            "8953                  yes and no. did you recognize him?  START no. i only saw his back. he went down an...\n",
            "5667                           trubshaw again? what now?      START snuff.  i must insist you try some. END\n",
            "13698                     i do not know what that means.   START huckleberry hound! what, are you nuts? END\n",
            "356           you say asia can be found by sailing west?  START yes, your eminence.  the voyage should n...\n",
            "10514                      i do not know the finish yet.  START well, go on, tell it. maybe one will com...\n",
            "---\n",
            "Maximum Question's Length:  38\n",
            "Maximum Answer's Length:  44\n",
            "Encoder token size is 16378 and decoder token size is 16211\n",
            "Decoder Token size after zero padding:  16212\n",
            "Train data size is:  (12000,)\n",
            "Test data size is :  (3000,)\n",
            "---\n",
            "Printing Model Summary...\n",
            "---\n",
            "Model: \"functional_31\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_21 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_22 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_10 (Embedding)        (None, None, 256)    4192768     input_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_11 (Embedding)        (None, None, 256)    4150272     input_22[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_10 (LSTM)                  [(None, 256), (None, 525312      embedding_10[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_11 (LSTM)                  [(None, None, 256),  525312      embedding_11[0][0]               \n",
            "                                                                 lstm_10[0][1]                    \n",
            "                                                                 lstm_10[0][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, None, 16212)  4166484     lstm_11[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 13,560,148\n",
            "Trainable params: 13,560,148\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Loading the Model weights...\n",
            "---\n",
            "FIVE EXAMPLES: TRAIN SENTENCE PREDICTIONS: \n",
            "---\n",
            "Question           : can we make this quick?  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad.  again.\n",
            "Actual response    : well, i thought we would start with pronunciation, if that is okay with you.\n",
            "Predicted response :  well, we will not to talk to the king, that he\n",
            "---\n",
            "Question           : well, i thought we would start with pronunciation, if that is okay with you.\n",
            "Actual response    : not the hacking and gagging and spitting part.  please.\n",
            "Predicted response :  not the hacking and gagging and spitting part. ple\n",
            "---\n",
            "Question           : not the hacking and gagging and spitting part.  please.\n",
            "Actual response    : okay... then how about we try out some french cuisine.  saturday?  night?\n",
            "Predicted response :  okay... then how about we stop the first time you \n",
            "---\n",
            "Question           : you are asking me out.  that is so cute. that is your name again?\n",
            "Actual response    : forget it.\n",
            "Predicted response :  forget it.\n",
            "---\n",
            "Question           : the thing is, cameron  i am at the mercy of a particularly hideous breed of loser.  my sister.  i cannot date until she does.\n",
            "Actual response    : seems like she could get a date easy enough...\n",
            "Predicted response :  seems like she could get a date easy enough...\n",
            "---\n",
            "FIVE EXAMPLES: TEST SENTENCE PREDICTIONS: \n",
            "---\n",
            "Question           : but how do you know you exist?\n",
            "Actual response    : it is intuitively obvious.\n",
            "Predicted response :  would i want to meet you for your own hand off,\n",
            "---\n",
            "Question           : it is intuitively obvious.\n",
            "Actual response    : intuition is no proof. what concrete evidence do you have of your own existence?\n",
            "Predicted response :  it is not his fault, evan. you were alive\n",
            "---\n",
            "Question           : intuition is no proof. what concrete evidence do you have of your own existence?\n",
            "Actual response    : hmm... well, i think, therefore i am.\n",
            "Predicted response :  i do not want to know about treadstone.\n",
            "---\n",
            "Question           : hmm... well, i think, therefore i am.\n",
            "Actual response    : that is good. very good. now then, how do you know that anything else exists?\n",
            "Predicted response :  very good, pris.\n",
            "---\n",
            "Question           : that is good. very good. now then, how do you know that anything else exists?\n",
            "Actual response    : my sensory apparatus reveals it to me.\n",
            "Predicted response :  we are from it, homerthis ai not but...\n",
            "---\n",
            "Importing libraries to calculate bleu score...\n",
            "---\n",
            "Calculating Bleu Score for Train data...\n",
            "---\n",
            "Bleu Score for Train data:  0.5716173872526239\n",
            "---\n",
            "Calculating Bleu Score for Test data...\n",
            "---\n",
            "Bleu Score for Test data:  0.06529228583260879\n",
            "---\n",
            "END!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STrC6-mQpuJh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}